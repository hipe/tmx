# Requirements and design objectives for the algorithm/schema

We're going to avoid writing a formal data-model here, because we wrote that
thing to allow the schema to *live* *in* a GraphViz document. So you should
be able to open the GraphViz document to see it (where? EDIT).

Against all odds, we found a way to generate an audit trail for one entity
efficiently, in a streaming way without writing to tempfiles. (This was made
much easier by the presence of the `-L` option in `git log`.) But as we're
about to see, this only gets us halfway there.

Similar (but not the same) to the design objectives in our efforts over in
[kiss-rdb] for entity audit-trails, we have these design objectives/
requirements:

- **Keep it as simple as possible** to the extent we can while meeting
   the remaining objectives.
- **No tempfiles**. This is why we introduced sqlite3.
- **Support progressive updates** with as little latency as is reasonable.
   (We expand on this point below.)
- **Integrate statistics somehow**. More below.

We have the entity audit-trail which is great. But imagine what happens
when we generate for the whole collection: Imagine we have 100 documents
each made up of an average of 3 notecards. In a coarse attempt, we would
traverse across every document (from a "big index") then traverse every
notecard within each document, then with each notecard, retrieve its full
history.

Then, in-memory somehow, for each document we have to synthesize what the
edit history is for that document from the edit histories of each current
notecard.

It's too much work to do each time we generate the site. So our sqlite
database is there to persist whatever work we can do up-front.

Ideally, there would be a quick check we could do (an `is_stale` column
on a `notecard` table, we just thought of, see below) to know whether
we need to build more audit trail


## First pass: add paths to a queue given the unindexed commits

Here's how it could work:

Once per site generation (or when an "update" CLI command/API call is made),

> Find out what the most recent SHA (commit) is of **the whole collection**
> with an appropriate call, something like:
>
> `git log -1 --format:tformat:blah -- path/to/notecards/entities`
>
> If this commit is in your `commit` table, you're up to date. Otherwise,
> you need to update.

We just realized you could combine the above check with the below. So:

> We want a stream of the commits made to the collection, most recent first,
> going back in time forever until we say stop. In each commit we want to
> know what files were affected. It's easy:
>
> `git log --numstat -- path/to/notecards/entities`

In each commit as it is displayed by `git-log` here, we have the same
datapoints we used in [kiss-rdb] to create the audit trail, things like
SHA and author and so on. This is relevant in a few beats below.

`git` produces the above in a memory-efficient streaming manner, so it's not
a waste of resources if we terminate the process mid-stream.

Now, the way we know whether or not we've seen ("indexed") each next
commit in this stream is by looking up its SHA in the `commit` table.

Let's sketch out that table now:

> The `commit` table has things like the SHA, the datetime (read up on
> type here), the author (string, as-is, for now) and the commit message.
> Almost definitely we don't want to mess with natural keys, just so our
> query construction is always self-consistent so this table will have a
> primary key separate from the SHA (which will be unique).

So, if the most recent commit (a.k.a "HEAD") is already in your `commit`
table, you are up to date and you are done. But otherwise:

> Some subset of the files (hopefully all of them) in the body of the
> commit listing are eno files that each correspond to the 0-N entities
> stored within in them at HEAD.

At first we were gonna say use a temporary table, but it will be nicer
for this to be persistent for when things fail mid-indexing:

> A `files_changed_queue` table with only two columns: primary key
> and `path`. `path` should be the path exactly as `git` sees it, relative
> to the project root. Wait, add one more column: `does_exist`.

Now,

> For each commit in the above stream produce by `git-log`,
> if the commit is already in the `commit` table, you have found
> an early stopping point. You are done with this first pass.
> Don't forget to terminate the `git-log` process.
>
> Otherwise, for each file listed in the body of this commit page
> (remember we used `--numstat`):
>
> If it is already in the `files_changed_queue` table by name, continue.
>
> Otherwise, insert a new record while populating the `does_exist` field
> with a hit to the fileystem. (We might scratch this, don't know why we're
> persisting it. But we elaborate below.)
>
> Now that you have updated the `files_changed_queue` by adding these zero
> or more filenames, we can sign-off with saying we have "seen" the commit
> by populating the `commit` table with all the component values of this
> commit (which you would have already parsed with your clever parser thing
> and stored in memory before you got to the body of the commit page which
> listed the files). So, insert into the `commit` table the SHA, datetime,
> author and cleaned-up message.


## Second pass: flush the "files that changed" queue

**NOW**, finally, we can start on the fun part: In the `files_changed_queue`,
we have a list of zero or more eno entity files that (at some point in our
history) changed without us indexing the change into this data warehouse.

It's possible (edge-casey, yes, but also an actual real, experienced thing)
that the `git-log --numstat` listed pathnames for files that no longer
exist ("under that name", if you like).

Yes, `--numstat` shows little numbers from which we can derive if it was a
delete; but we would have to follow the history in its entirety from present
to rebuild this, just to ascertain something we can ascertain by a hit to
the filesystem. Again, we may discard this filesystem hit here because below
we'll hit it agian to read the same file.

OK, So:

> For each path in this table, if it doesn't exist on the filesystem (per
> the dedicated field about this in the table), there is no work to do and
> it no longer belongs in the queue at this phase. Delete the record and
> continue.
>
> Otherwise (and the row corresponds to an existant file, last we checked),
>
> Parse the file with our eno storage adapter.
>
> (If it fails to parse, error-handling would be nice but skip for now.)
>
> For each entity section in the file, it states an EID.
>
> If the EID corresponds to a row already in the `notecard` table,
> update the `is_stale` field of that row to be `true` (i.e `1`).
> (We may be redundantly setting it to `true`; we won't bother checking.)
>
> Otherwise, insert a new record into the EID table with the appropriate
> values (including `is_stale: 1`).
>
> Once you have updated for all the notecards in the file, delete this
> file's row from the `files_changed_queue` table. (UH-OH - can we traverse
> the table while deleting from it?? what will happpeeennnnnn????)

It's worth mentioning that this scenario can happen: You may have deleted
an entity which could cause its file to show up in Pass 1, but because the
entity no longer exists in the file, it's as if it never existed at all and
so in Pass 2 nothing happens in regards to this entity (and perhaps in
regards to this file).

For example, maybe you deleted entity `ABC` from the file `entities/A/B.eno`
in the relevant history, and so this file shows up in the `git log` that we
do for this update.

When we go to ask the file `entities/A/B.eno` what entities it has, it will
*not* say `ABC`, because that entity is no longer in the file. This will be
the end of it. There will be no gathering of history for `ABC`. This is by
KISS design that when an entity is deleted it's as if it never existed.

A workaround would be to create a "deactivate" feature and have an
`is_active` field. But yeah, we don't want to bend our KISS ways to
accomodate this use case of generating history on deleted entities.

This ends the second pass. We should now have an empty `files_changed_queue`
and our `notecard` has zero or more rows in it flagged as `is_stale`.

While we're on the subject, we never defined the `notecard` table:

> The `notecard` table needs only a primary key field, an `entity_identifier`
> field which can be indexed and unique, and an `is_stale` column.


## Third pass: unstale the stales

We'll be entering our third pass now which involves traversing every
`notecard` row marked as stale and using our existing "audit trail" feature,
and with that stream of data, writing something to some tables somehow.

We'll mention now that it's possible (frequent, evne) that a notecard was
flagged as stale (because it lives in a file that had new changes) but in
fact the notecard will be already indexed. We will detect this on the first
"edit" component of its audit trail.

As it stands, depsite this "waste", using our existing audit trail facility
in conjuncion with `git log -P` still seems like the best way to detect such
cases.

And now, here's a provision/assertion we'll encounter later that we'll
state explicitly here:

> ### provision:
>
> Since (above) we started at HEAD and went back in time, adding more rows
> to the `commit` table until we found a commit we've already seen; we should
> now have every commit "ever" (as it pertains to the notecard collection) in
> the `commit` table.
>
> The only way this could be not true is if something weird happened involving
> moving a versioned file in to that tree when it wasn't there before, but
> we're gonna just hope we don't enounter that for now.

As such, when we encounter commits in the audit trail, we should assert
we already have a record of the commit in our `commit` table.

OK, so what data do we want to gather?

> The `notecard_commit` table has a foreign key to the `notecard` table
> and a foreign key to the `commit` table. Additionally it has (and this
> part is very subject to change) for now, we'll just keep a `verb` column
> (explained below), and a `number_of_lines_inserted` column and a
> `number_of_lines_deleted` column

Something just occurred to us: how we measure change will have a big
blindspot.

Using our exiting [kiss-rdb] "audit trail" facility, we can either see the
raw diff line to the entity file (just as it pertains to the entity), or
we can see the diff of the change to the `body` field (if any).

However, when a notecard gets added to a document, this is typically a lot of
change from the perspective of what content is in the document; but it appears
as a relatively small amount of change from the perspective of the notecard.

(Indeed this was sort of the point of [pho] was that moving notecards
around was a cheap and easy.)

But what it all comes down to is everything will be fine: Under this metric
for change (and this family of algorithms), the document history is simply
an aggregation of the history of the notecards it holds at HEAD. And the
datetimes associated with the edits are associated with the edits of *content*
in the *notecards*, which is arguably a more interesting and precise metric,
reflecting the full history of the notecard going back to its birth.

OK so:

> For each notecard in the `notecard` table flagged as stale, get its audit
> trial stream using our facility in [kiss-rdb]. Hold on to the primary
> key of the notecard.
>
> For each `entity_edit` component of that trail, get its 'SHA':
>
> Assert that a `commit` already exists in the `commit` table for this commit
> referenced in this audit trail component (as justified above) and get its
> primary key. Hold on to its primary key.
>
> See if a record already exists in the `notecard_commit` table for this
> notecard and this commit. If one does, you have reached the end of
> processing this audit trail. Terminate its process somehow. Otherwise:
>
> Populate the `notecard_commit` table with the two foreign keys and also
> the number of lines added and number of lines removed (over all, not of
> the `body` field (although we could store that in another pair of columns
> trivially). Additionaly populate the `verb` field with either `CREATE`
> or `CHANGE`. (If it's easier, just use a sqlite UPDATE on the last created
> row once you reach the end of the audit trail.)
>
> Once you reach the end of the audit trail for this notecard (or you exited
> the traversal early per above), update the `notecard` table to say that
> this notecard is not stale.

When you successfully complete the above, no more notecards are stale,
all notecard commits have been indexed, and all commits to the notecards
collection have been indexed, between HEAD and the beginning of time.


## Fourth pass:

> For every row of the `notecard_commit` table that has a `state` of
> `PREPARED`, hold on to the commit ID. Hold on to the whole row, actually.
>
> Find what document the notecard is a part of (if any) using the kiss-rdb.
>
> (If no document, skip to the end of this section.)
>
> We'll have a `notecard_based_document` table with the EID of the head
> notecard.
>
> We'll have a `notecard_based_document_commit` table that has foreign keys
> to those two. And `number_of_lines_inserted` and `number_of_lines_deleted`.
> And `number_of_notecards`.
>
> Find an existing `notecard_based_document` record or insert one if it
> doesn't exist, and hold on to its ID.
>
> Find an existing `notecard_based_document_commit` record if any.
>
> If one does not exist, insert it with the two foreign keys, and zeros
> for the three counting fields.
>
> Now, IN ONE ATOMIC TRANSACTION if you can, update the last mentioned
> table: update the three counter fields using addition: increment that
> one field by 1, and the other two fields, increment them by the numbers
> in the `notecard_commit`. Additionally in this same transaction, change
> the status of the `notecard_commit` row to `INDEXED`.
>
> If the `notecard_commit` wasn't associated with a document, still you have
> to update its status to `INDEXED`.

NOW: you can query the `notecard_commit` table to build the document history.
Join to the `commit` table, sort by datetime of the commit. Assert that
there are only lines inserted, non deleted, on the oldest/first commit (and
note that the oldest commit "under" the document, there's no reason to expect
it's gonna be a commit that touches the head notecard. BUT THERE'S ONLY ONE
THING MISSING: STATISTICS!


## Fifth pass (period, probably): statistics

The real fun is in deriving "semantic boundaries" around different sizes,
based purely on statistics. We have a rough idea how we want to do this,
where the "amount" of change in a document commit is merely the sum of the
amount of lines inserted plus the the of lines deleted.

- (Don't count "CREATE" verbs here which should be the same as the
  earliest commit for this document).
- (We could sophisticate how we measure change, but this simple summing
  is good enough for us for now.)

We think we want to calculate the standard deviation of these numbers,
and somehow use that to form the numeric boundaries around linguistic
categories (`verb_lexeme_key` like "small edit", "edit", "big edit").

> Our `statistics` table would have `statistic_name` and `value`

(We considered having a `start` and `stop` but not nec)

Probably we don't want the extra moving parts to have our statistics
change every time we update. (This is a quite high-level, config-y type
choice: because do you really want "what words mean" to "rewrite history"
spuriously, when you etc.)



# Caveats:

If you rewrite history in your repo (git rebase...)



# (document-meta)

  - #born
