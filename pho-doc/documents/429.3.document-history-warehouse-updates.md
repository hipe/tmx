# Requirements and design objectives for the algorithm/schema

We're going to avoid writing a formal data-model here, because we wrote that
thing to allow the schema to *live* *in* a GraphViz document. So you should
be able to open the GraphViz document to see it (where? EDIT).

Against all odds, we found a way to generate an audit trail for one entity
efficiently, in a streaming way without writing to tempfiles. (This was made
much easier by the presence of the `-L` option in `git log`.) But as we're
about to see, this only gets us halfway there.

Similar (but not the same) to the design objectives in our efforts over in
[kiss-rdb] for entity audit-trails, we have these design objectives/
requirements:

- **Keep it as simple as possible** to the extent we can while meeting
   the remaining objectives.
- **No tempfiles**. This is why we introduced sqlite3.
- **Support progressive updates** with as little latency as is reasonable.
   (We expand on this point below.)
- **Integrate statistics somehow**. More below.

We have the entity audit-trail which is great. But imagine what happens
when we generate for the whole collection: Imagine we have 100 documents
each made up of an average of 3 notecards. In a coarse attempt, we would
traverse across every document (from a "big index") then traverse every
notecard within each document, then with each notecard, retrieve its full
history.

Then, in-memory somehow, for each document we have to synthesize what the
edit history is for that document from the edit histories of each current
notecard.

It's too much work to do each time we generate the site. So our sqlite
database is there to persist whatever work we can do up-front.

Ideally, there would be a quick check we could do (an `is_stale` column
on a `notecard` table, we just thought of, see below) to know whether
we need to build more audit trail


## First pass: add paths to a queue given the unindexed commits

Here's how it could work:

Once per site generation (or when an "update" CLI command/API call is made),

> Find out what the most recent SHA (commit) is of **the whole collection**
> with an appropriate call, something like:
>
> `git log -1 --format:tformat:blah -- path/to/notecards/entities`
>
> If this commit is in your `commit` table, you're up to date. Otherwise,
> you need to update.

We just realized you could combine the above check with the below. So:

> We want a stream of the commits made to the collection, most recent first,
> going back in time forever until we say stop. In each commit we want to
> know what files were affected. It's easy:
>
> `git log --numstat -- path/to/notecards/entities`

In each commit as it is displayed by `git-log` here, we have the same
datapoints we used in [kiss-rdb] to create the audit trail, things like
SHA and author and so on. This is relevant in a few beats below.

`git` produces the above in a memory-efficient streaming manner, so it's not
a waste of resources if we terminate the process mid-stream.

Now, the way we know whether or not we've seen ("indexed") each next
commit in this stream is by looking up its SHA in the `commit` table.

Let's sketch out that table now:

> The `commit` table has things like the SHA, the datetime (read up on
> type here), the author (string, as-is, for now) and the commit message.
> Almost definitely we don't want to mess with natural keys, just so our
> query construction is always self-consistent so this table will have a
> primary key separate from the SHA (which will be unique).

So, if the most recent commit (a.k.a "HEAD") is already in your `commit`
table, you are up to date and you are done. But otherwise:

> Some subset of the files (hopefully all of them) in the body of the
> commit listing are eno files that each correspond to the 0-N entities
> stored within in them at HEAD.

At first we were gonna say use a temporary table, but it will be nicer
for this to be persistent for when things fail mid-indexing:

> A `changed_file_queue` table with only two columns: primary key
> and `path`. `path` should be the path exactly as `git` sees it, relative
> to the project root. Wait, add one more column: `does_exist`.

Now,

> For each commit in the above stream produce by `git-log`,
> if the commit is already in the `commit` table, you have found
> an early stopping point. You are done with this first pass.
> Don't forget to terminate the `git-log` process.
>
> Otherwise, for each file listed in the body of this commit page
> (remember we used `--numstat`):


(NOTE we'll modify this algorithm below but this still lays a foundation.)


> If it is already in the `changed_file_queue` table by name, continue.
>
> Otherwise, insert a new record while populating the `does_exist` field
> with a hit to the fileystem. (We might scratch this, don't know why we're
> persisting it. But we elaborate below.)
>
> Now that you have updated the `changed_file_queue` by adding these zero
> or more filenames, we can sign-off with saying we have "seen" the commit
> by populating the `commit` table with all the component values of this
> commit (which you would have already parsed with your clever parser thing
> and stored in memory before you got to the body of the commit page which
> listed the files). So, insert into the `commit` table the SHA, datetime,
> author and cleaned-up message.

There's a complication we must introduce: The problem with the above is
that it adds commits to the `commit` table in the order it encounters
them, which is the order we get from `git-log`, which is starting from
HEAD and going back in time. The problem is if something fails between
us processing the HEAD commit and the earliest commit we need to index
(you should absolutely plan for this), then our check for whether we are
all caught up will give a false-positive (it will see that HEAD is indexed
and think that we are done.)

Our solution is to write SHA's into a temporary table one by one, starting
from HEAD and going back in time until you find a commit already in the
`commit` table. Hopefully, you can do this much without failing (hence
why this table is temporary). Once you reach this point, treating this
temporary table like a stack, pop the most recently added item off it,
(which will be the "oldest"), process *this* commit (only) in the manner
described above (update the `changed_file_queue`, add to the `commit`
table), (either do another `git log` on a same commit, one-by-one (this time
`git log -1`) or make the temporary table complicated) then pop this SHA out
of the temporary table.

When this pass is finished, `changed_file_queue` is populated with the
pending queue of paths that need to be processed, and `commit` has the
most recent relevant commit.

Some little points:

- For most of our use cases, we could just read the SHA's into memory
  of the python runime rather than use a temporary table; but where's
  the fun in that?
- We put "oldest" (and "newest") in quotes but, it's essential to mention
  somewhere that we must never rely on commit datetimes to represent the
  order of commits in the repository timeline. The datetimes are not the
  authoritative source for timeline order, the SHA's as presented by
  `git log` are. The datetimes need not correspond to timeline order.
  A quick demonstration with `git rebase -i` and reordering commits will
  demonstrate this.




## Second pass: flush the "files that changed" queue

**NOW**, finally, we can start on the fun part: In the `changed_file_queue`,
we have a list of zero or more eno entity files that (at some point in our
history) changed without us indexing the change into this data warehouse.

It's possible (edge-casey, yes, but also an actual real, experienced thing)
that the `git-log --numstat` listed pathnames for files that no longer
exist ("under that name", if you like).

Yes, `--numstat` shows little numbers from which we can derive if it was a
delete; but we would have to follow the history in its entirety from present
to rebuild this, just to ascertain something we can ascertain by a hit to
the filesystem. Again, we may discard this filesystem hit here because below
we'll hit it agian to read the same file.

OK, So:

> For each path in this table, if it doesn't exist on the filesystem (per
> the dedicated field about this in the table), there is no work to do and
> it no longer belongs in the queue at this phase. Delete the record and
> continue.
>
> Otherwise (and the row corresponds to an existant file, last we checked),
>
> Parse the file with our eno storage adapter.
>
> (If it fails to parse, error-handling would be nice but skip for now.)
>
> For each entity section in the file, it states an EID.
>
> If the EID corresponds to a row already in the `notecard` table,
> update the `is_stale` field of that row to be `true` (i.e `1`).
> (We may be redundantly setting it to `true`; we won't bother checking.)
>
> Otherwise, insert a new record into the EID table with the appropriate
> values (including `is_stale: 1`).
>
> Once you have updated for all the notecards in the file, delete this
> file's row from the `changed_file_queue` table. (UH-OH - can we traverse
> the table while deleting from it?? what will happpeeennnnnn????)

It's worth mentioning that this scenario can happen: You may have deleted
an entity which could cause its file to show up in Pass 1, but because the
entity no longer exists in the file, it's as if it never existed at all and
so in Pass 2 nothing happens in regards to this entity (and perhaps in
regards to this file).

(This theme of [EDIT] will be developed in to a provision below.)

For example, maybe you deleted entity `ABC` from the file `entities/A/B.eno`
in the relevant history, and so this file shows up in the `git log` that we
do for this update.

When we go to ask the file `entities/A/B.eno` what entities it has, it will
*not* say `ABC`, because that entity is no longer in the file. This will be
the end of it. There will be no gathering of history for `ABC`. This is by
KISS design that when an entity is deleted it's as if it never existed.

A workaround would be to create a "deactivate" feature and have an
`is_active` field. But yeah, we don't want to bend our KISS ways to
accomodate this use case of generating history on deleted entities.

This ends the second pass. We should now have an empty `changed_file_queue`
and our `notecard` has zero or more rows in it flagged as `is_stale`.

While we're on the subject, we never defined the `notecard` table:

> The `notecard` table needs only a primary key field, an `entity_identifier`
> field which can be indexed and unique, and an `is_stale` column.


## Third pass: unstale the stales

We'll be entering our third pass now which involves traversing every
`notecard` row marked as stale and using our existing "audit trail" feature,
and with that stream of data, writing something to some tables somehow.

We'll mention now that it's possible (frequent, even) that a notecard was
flagged as stale (because it lives in a file that had new changes) but in
fact the notecard will be already indexed. We will detect this on the first
"edit" component of its audit trail.

As it stands, depsite this "waste", using our existing audit trail facility
in conjuncion with `git log -P` still seems like the best way to detect such
cases.

And now, here's a provision/assertion we'll encounter later that we'll
state explicitly here:

> ### provision:
>
> Since (above) we started at HEAD and went back in time, adding more rows
> to the `commit` table until we found a commit we've already seen; we should
> now have every commit "ever" (as it pertains to the notecard collection) in
> the `commit` table.
>
> The only way this could be not true is if something weird happened involving
> moving a versioned file in to that tree when it wasn't there before, but
> we're gonna just hope we don't enounter that for now.

(Narrator voice: the weird thing *did* happen and we hacked through it.)

As such, when we encounter commits in the audit trail, we should assert
we already have a record of the commit in our `commit` table.

OK, so what data do we want to gather?

> The `notecard_commit` table has a foreign key to the `notecard` table
> and a foreign key to the `commit` table. Additionally it has (and this
> part is very subject to change) for now, we'll just keep a `verb` column
> (explained below), and a `number_of_lines_inserted` column and a
> `number_of_lines_deleted` column

Something just occurred to us: how we measure change will have a big
blindspot. (Narrator voice: as it turned out, it was just fine.)

Using our existing [kiss-rdb] "audit trail" facility, we can either see the
raw diff line to the entity file (just as it pertains to the entity), or
we can see the diff of the change to the `body` field (if any).

However, when a notecard gets added to a document, this is typically a lot of
change from the perspective of what content is in the document; but it appears
as a relatively small amount of change from the perspective of the notecard.

(Indeed this was sort of the point of [pho] was that moving notecards
around was a cheap and easy.)

But what it all comes down to is everything will be fine: Under this metric
for change (and this family of algorithms), the document history is simply
an aggregation of the history of the notecards it holds at HEAD. And the
datetimes associated with the edits are associated with the edits of *content*
in the *notecards*, which is arguably a more interesting and precise metric,
reflecting the full history of the notecard going back to its birth.

OK so:

> For each notecard in the `notecard` table flagged as stale, get its audit
> trial stream using our facility in [kiss-rdb]. Hold on to the primary
> key of the notecard.
>
> For each `entity_edit` component of that trail, get its 'SHA':
>
> Assert that a `commit` already exists in the `commit` table for this commit
> referenced in this audit trail component (as justified above) and get its
> primary key. Hold on to its primary key.
>
> See if a record already exists in the `notecard_commit` table for this
> notecard and this commit. If one does, you have reached the end of
> processing this audit trail. Terminate its process somehow. Otherwise:
>
> Populate the `notecard_commit` table with the two foreign keys and also
> the number of lines added and number of lines removed (over all, not of
> the `body` field (although we could store that in another pair of columns
> trivially). Additionaly populate the `verb` field with either `CREATE`
> or `CHANGE`. (If it's easier, just use a sqlite UPDATE on the last created
> row once you reach the end of the audit trail.)
>
> Once you reach the end of the audit trail for this notecard (or you exited
> the traversal early per above), update the `notecard` table to say that
> this notecard is not stale.

When you successfully complete the above, no more notecards are stale,
all notecard commits have been indexed, and all commits to the notecards
collection have been indexed, between HEAD and the beginning of time.


## Fourth pass:

### introducing a persnickety design vector: document structure

The objective of the fourth pass is to populate the two tables (not
introduced yet) that will be used to generate (finally) the document
history summaries.

Our schema and hypothetical data so far fully represent all participating
"commits" and all participating "notecards" (right? .. anyway it does to
whatever extent it does).

And note it got from the one to the other by looking at "files" and expanding
them out to the entities in them at HEAD, and doing an audit trail on each
entity. This gave us the abstraction of a "notecard commit".

So we have commits, notecards, and notecard commits, but one thing we don't
have is any representation for *documents*. To build document histories,
we will need *documents* and *document commits*. Populating these two tables
is the objective of this pass (and introducing them is something we still
haven't done yet, formally).

The extra tricky part of this pass relates to something we'll form into a
provision now:


> ### provision:
>
> We don't ever go back in time to reconstruct the document (tree) structure
> as it once was. All the work of indexing for our end goal (document
> histories), where it depends on document structure, it will take
> the document structure as it exists on the filesystem at the time the
> indexing was performed.

This provision is adopted a sort of ... algorithmic occam's razor (EDIT
that's not quite right) because, to go back in time to reconstruct document
structure "correctly" has a benefit that is not justified by the cost.

From one perspective it's "imperfect" but from another perspective, it's sort
of a six-of-one, half-a-dozen-of-the-other trade-off: When you're asking for
the document history of a document built from notecards, should you derive
it from the notecards that are in that document *right now* or should the
document history actually be a true reflection of how that "document" looked
in the past? (It becomes a ship of Theseus problem: Is a document really the
same document if it has totally different notecards with different content,
except one notecard at the head?)

Yes of course purists want it to be the harder way, but in practice we're
hoping it will be just fine this way. It might even be better. And in any
case, checking out older versions of the repository while doing indexing would
be nasty in a lot of ways. So it's a really tall ask and heavy lift that's
not gonna make the cut.

But this provision has a massive corollary gotcha: If you take this big index
and then change your structure and then update the index; it will produce
different results than if you blow away the database and build the index
from scratch.

This should of course be unsuprising, because the point of the database is
to store things longer term that's too voluminous or costly to calculate at
runtime; but it comes with the tradeoff that we are basing it off something
that can be considered "fixed" (like commits), and we can update it only
when that fixed set of upstream data is "additive" (getting more commits
doesn't change old commits).

If document structure is not fixed and the database's data relies on document
structure, then things will get interesting when the document structure
changes.

The really "simple" "workaround" for this is to just say you have to rebuild
the whole document history from scratch any time you change document structure.

But that's an overkill solution, it doesn't scale, and it's sort of contrary
to the whole point of the database (the point of the database is to save us
from doing costly or voluminous calculations repeatedly).

As we develop the algorithm for populating these two final tables (not
introduced yet) in this pass, we'll keep an eye on this whole design vector
looming in the shadows.


### Introducing the tables

For starters,

> We'll have a `notecard_based_document` table with the EID of the head
> notecard.
>
> We'll have a `notecard_based_document_commit` table that has foreign keys
> to those two. And `number_of_lines_inserted` and `number_of_lines_deleted`.
> And `number_of_notecards`.

Note that (although one of those table joins out to the `commit` table),
those two tables hold the only data we use in creating the end goal: the
document history (except for statistics which is the next pass).

The second table is about a commit that affected the document. Okay.
But note that this is highly derived: If multiple documents were affected
in one commit (totally common), there would multiple records in this table
for one commit (one record for each document).

Also, those three "number of" columns are an aggregation: As it stands,
there is no single place we can go to look these numbers up. The number
of lines inserted etc for a "document commit" is the sum of the number
of lines inserted for each participating notecard that was changed at
that commit. Whew!


### Towards an algorithm

If mutable document structure weren't a thing, we would probably do something
like this:

Find every commit from the `commit` table not yet represented in the
document commit table. (We might use the MINUS (actually EXCEPT) set operator
here on the commit primary key, between the two tables.)

Iterate over every such commit (in some order?), and for each commit,
expand it out to its zero or more "notecard commits" though its straighforward
one-to-many join.

For our purposes, rather than seeing the above as a loop inside a loop,
we can just see it (and code it, if we want) as a stream of notecard commits
we haven't (in this pass) seen yet. (Actually, no.)

Each notecard commit points to a notecard which has an entity identifier
(notecard EID). Using a custom function exposed by [pho] we take a
"big index" (which we should build lazily, max once ever for this pass),
and determine which (if any) document (notecard) this notecard is a part of.

Both "document head" notecard and subject notecard should have a record in
the `notecard` table if they don't already. (Through some strange-but-possible
scenario, we might encounter a "document head" notecard (EID) we haven't seen
anywhere yet so "touch" that in to existence if necessary.)

Although [pho] document structure can be complex with arbitrarily deep child
nodes, with documents under documents arbitrarily deep, and with taxonomic
nodes ("dewey" nodes) above documents arbitrarily high; for our objective
here all we care about is that a notecard is either a parent (a document head),
a child of one of those parents, or an unaffiliated notecard.

We can distill all this down into a simple parent-child relationship
that we can keep in one table with only two columns, each a notecard ID.
The child ID in the table will be unique (you can only have one parent)
so we can use it as the primary key of these records too.

We'll keep this in a temporary table to reinforce the fact that
document structure is volatile (but see our provision above and all the
corollaries).

OK so, now we know what "document" (head notecard) this notecard is a part
of. It could be the head notecard itself. It might be unafilliated, in which
case we can probably just skip.

Touch a "notecard based document" into existence with the head notecard EID
if a record of one doesn't already exist.

The place we want to put the data of the subject "notecard commit" is
the `notecard_based_document_commit` table. Touch one in to existence.
This is key: we may have seen this record before, if we have seen another
notecard commit that was of this same commit, but a different notecard
but a notecard of this document.

That's confusing as hell, so, put it another way: Maybe when we were
authoring, we had one commit that changed several notecards that were all
part of the same document. What we want right now is to make it look like it
was just one "document commit" with one total set of numbers (of number of
lines added, number of lines removed etc). What we're doing right now is
adding those numbers up piece-by-piece as we encounter them.

Here's an overview of the hypothetical algorithm:

> For every `notecard_commit` produced by [the stream described above],
>
> Find what document the notecard is a part of (if any) using the kiss-rdb.
>
> (If no document, skip to the end of this section.)
>
> Find an existing `notecard_based_document` record or insert one if it
> doesn't exist, and hold on to its ID.
>
> Find an existing `notecard_based_document_commit` record if any.
>
> If one does not exist, insert it with the two foreign keys, and zeros
> for the three counting fields.
>
> Now, IN ONE ATOMIC TRANSACTION if you can, update the last mentioned
> table: update the three counter fields using addition: increment that
> one field by 1, and the other two fields, increment them by the numbers
> in the `notecard_commit`. Additionally in this same transaction, change
> the status of the `notecard_commit` row to `INDEXED`.
>
> If the `notecard_commit` wasn't associated with a document, still you have
> to update its status to `INDEXED`.

Here's an expanded version of the above with some additions we thought of
today:

> For each primary key ID (in any order) that's a result of the query
> of every commit ID that's not in the document commit table
>
> (probably something like:
>     SELECT commit_ID from commit EXCEPT
>     SELECT DISTINCT commit_ID FROM document_commit
>  but we're not sure, we've never done set operations in SQL before)
>
> For each such ID (of a commit),
> Get the list of zero or more notecard commits associated with that commit.
> Just read them all in to memory, it's fine.
>
> This set of zero or more notecard commits represents ALL the notecards that
> will go in to making ONE or more "document commits" for this commit.
> We know one such document commit doesn't exist yet because we did the
> set difference operation above. (How/why we would make one from zero we
> will discuss below.)
>
> (note we WILL be putting things in transactions but we haven't started
> yet and we will indicate when we do.)
>
> At the start of this pass we should do two things:
> 1) Create the parent-child temporary table we'll describe in a bit
> 2) Create a lazy memoization of the "big index" from [pho]
>
> Any given notecard is guaranteed not to show up more than once in this
> list of "notecard commits". Note we still aren't in a transaction yet.
>
> So, for the list (and set) of zero or more participating notecards here,
> (and you can either do this in one traversal or several; the N should never
> be very large),
>
> For each such notecard:
>
>   1) Determine what (if any) document it's a part of. It could be:
>      1) Part of *no* document (unaffiliated)
>      2) Part of a document headed by a different notecard
>      3) Itself the head of a document
>
>      For (2) and (3) we will touch records in a TEMPORARY `notecard_parent`
>      table:
>        - Give the table only two columns: `notecard_ID` and
>          `document_head_notecard_ID`.
>        - When the notecard is itself the document head,
>          put the same PK ID in both cells.
>        - We may have already seen this notecard in another notecard commit
>          in this pass, in which case that ID is already in this table.
>          Check first, which is what we meant by "touch". [pho] manages a
>          tree structure for the notecards; any given notecard can have max
>          one parent. Although the parent can change over time, it need not
>          be considered volatile for the lifetime of this pass.
>
>      For (1) (and the notecard was not part of a document) see next:
>
>  2) "touch" a record for the document (if any) in the
>     `notecard_based_document` table. NOW, IF THE NOTECARD was not a part
>     of any document, TOUCH THE imaginary document headed by imaginary
>     notecard with EID `222`. This is the number zero as an entity
>     identifier, and it's a cute way of saying "NULL". (we should be able to
>     insert "" (the empty string) instead, but aesthetically this just
>     looks better.) The reason we do this is bit of a hack we're willing to
>     live with which we describe below. NOTE WE STILL HAVEN'T started the
>     transaction yet!
>
> OK, Once you have done the above, NOW start the transaction:
>
> For each notecard commit here (again),
>
> 1) Look up its parent (notecard) in the temporary table. If there is one,
>    look up the existing record in notecard based document (already created
>    above.) If there isn't use, USE THE ID FOR THE `222` DOCUMENT.
>    (OR MAYBE USE `notecard_based_document` IDs HERE)
> 2) Touch a record in the `notecard_based_document_commit` table. One will
>    already exist IFF you have already passed another notecard that was part
>    of this document and also this commit. If the notecard is not a part of
>    a document, leave the count fields as zeroes. Otherwise, increment them
>    with the numbers from this notecard commit.
>
> NOW, once you have traversed all the notecard commits in this commit in
> the above loop, go ahead and commit the transaction.
>
> The reason we use this imaginary document `222` in this way is so that
> *every* commit we look at ends up with an entry in the
> `notecard_based_document_commit` table, even those with no participating
> notecards. Otherwise our set difference operation at the beginning of this
> block would keep looking at non-participating commits over and over again
> every time we update. It's kind of a hack but it's in the spirit of
> parsimony and fewer moving parts.
>
> WHEW!!!



NOW: you can query the `notecard_commit` table to build the document history.
Join to the `commit` table, sort by datetime of the commit. Assert that
there are only lines inserted, non deleted, on the oldest/first commit (and
note that the oldest commit "under" the document, there's no reason to expect
it's gonna be a commit that touches the head notecard. BUT THERE'S ONLY ONE
THING MISSING: STATISTICS!


## Fifth pass (period, probably): statistics

The real fun is in deriving "semantic boundaries" around different sizes,
based purely on statistics. We have a rough idea how we want to do this,
where the "amount" of change in a document commit is merely the sum of the
amount of lines inserted plus the the of lines deleted.

- (Don't count "CREATE" verbs here which should be the same as the
  earliest commit for this document).
- (We could sophisticate how we measure change, but this simple summing
  is good enough for us for now.)

We think we want to calculate the standard deviation of these numbers,
and somehow use that to form the numeric boundaries around linguistic
categories (`verb_lexeme_key` like "small edit", "edit", "big edit").

> Our `statistics` table would have `statistic_name` and `value`

(We considered having a `start` and `stop` but not nec)

Probably we don't want the extra moving parts to have our statistics
change every time we update. (This is a quite high-level, config-y type
choice: because do you really want "what words mean" to "rewrite history"
spuriously, when you etc.)



# Four-point-five: rigged documents

(quick sketch)

This concern isn't given as much attention because it isn't as interesting
but it is still a lot of work.

  - Mostly a separate data space for now, from all of the above
  - Singleton table has most recently seen SHA *for this purpose*
  - If not out of date, skip down to #here1 below
  - From the big index you can get a set of every participating file (hash)
  - Merge this against a dedicated table, giving each entry a state of
    'added' or 'missing' (or leave a state of 'exists' or 'stale' as-is).
  - If, after this, any are missing, maybe exit with complaint
  - Since the maximum practical imaginable size of this set of paths won't
    ever be impermissably huge (famous last words), we can just read them
    all into one big hash (values not important) (yes it is see below).
    (maybe skip the stales)
  - "long walk" from head commit back to last seen SHA or beginning of time,
    (use the same `git-log --numstat` from above), traversing over the body
    of each commit (as reported here) doing a set intersect with the hash from
    above. for each match that is not already true in the hash, flip the
    `state` field in the table to `stale` and commit and set it to true in the
    hash. At the end of the 'long walk', update the SHA in the singleton table.
  - :#here1: now, our remaining work is for all rigged documents that have
    been flagged as 'stale'. This table will have a `last_indexed_SHA` field.
    If we have none, we are done! Otherwise, we'll make a temporary table:
    For each stale document, do the new "audit trail" for rigged documents,
    at each commit, add a record to the temporary table until you reach the
    last indexed SHA or the first ever commit for the document.
  - "transfer" records from the temporary table to the dedicated, permanent
    table. update the `last_indexed_SHA` field in the other table.
    update the `state` field to 'exists'. commit.
  - Repeat the above for each rigged document (in the table) marked as stale.
    When no more are marked stale, you are done!



# Caveats:

If you rewrite history in your repo (git rebase...)



# (document-meta)

  - #history-B.4 spike more
  - #born
