# tmx doc-test

## objective & scope

the subject sidesystem is an experimental for-fun tool that generates
tests from comments. it is inspired by python's "doc-test" tool
(although we have held off from looking at that project beyond a cursory
glance in the interest of coming up with ideas in a vacuum and only
afterwards comparing them to whatever else is out there (and then
variously kicking ourselves and lauding ourselves as appropriate)).


### synopsis

if your "asset document" (code file) contains a snippet like this:

    this is some code

    # we have redefined what it means to add (don't actually do this).
    # adding two positive integers produces their sum plus one:
    #
    #     1 + 2  # => 4


the subject sidesystem produces this:

    it "adding two positive integers produces their sum plus one" do
      expect( 1 + 2 ).to eql 4
    end


for more complete working examples, see (EDIT).

for an introduction to the nitty gritty details about how this works and
how it can be used, see below near "towards a deeper understanding of the
translation syntax".



### brief feature overview

  • different test-suite solutions can be targeted individually
    through its adapter architecture.

  • uses a *really* simple mustache-like form of template

  • (NEW) "forwards" and "reverse" "synchronization"! (see far below).



### current limitations

because both a) this is largely a proof-of-concept and b) to do this
"right" is outside our current scope of interest for this platform,
this entire project is at its essence a hack: we "parse" code files and
test files in this project, but we do *not* do so the "right" way, so edge
cases will certainly exist where the subject sidesystem will fail.
(this is discussed further in [#019] current limitations in parsing.)



### general limitations

we do not proffer that the sort of tests generated by this sidesystem
should serve as a replacement for unit tests. what makes good unit tests
and what makes good documentation examples are largely a different set
of characteristics (depending of course on whom you ask):

  • perhaps unit tests should take small steps that are good for regression.
    maybe they should even be [#sl-129] "three rules" compliant.

  • perhaps good documentation examples should focus on giving a
    high-level overview of key features.

(but note that as we write this we are in the middle of a rewrite that
is meant to make these two worlds coalesce somewhat #open [#001]..)



### approach

sensible defaults, zero config. ("the target document is the config.")




## towards a deeper understanding of the translation "syntax"

to understand how to get the subject sidesystem to do exactly what you
want (to the extent that that is possible), it may be useful to
understand the general pipeline described here for translating asset
documents into test documents.

### a document is broken into blocks

at the coarsest level, the input (which is internally modeled as a line
stream, but imagine it is is a file (i.e an "asset document")), this
input line stream is broken into "blocks". each block is either a
"comment block" or a "static block".

static blocks typically contain your real code and are ignored, but
comment blocks are scanned for certain patterns. see [#020]
"what are comment blocks?" for more on this early stage of the pipeline.

### blocks are broken into runs

then we break each comment block down further into groupings of
associated lines we call "runs". each run is either a "discussion run"
or a "code run." typically you explain something in more or less natural
language in a "discussion run", and a "code run" has example code
demonstrating the thing you just explained.

the boundary between discussion run and code run is detected by
matching a pattern of change in indent. this pattern is generally
intuitive and easy to read and write and more or less follows the rule
of thumb that increase indent by four (4) spaces will transition you
from discussion run to code run; and then if you decrease the indent
back to your old margin, you'll go back to a discussion run.

(despite how simple this may sound, the inner-workings are a bit
involved being that they rely on detecting what the "baseline" margin is
as well as detecting when this changes, apart from detecting when the
current margin has exceeded this threshold. all of this fun is the
subject of [#021] "what are runs?".)

### runs make items and other nodes

finally, a pairing of dicsucssion run followed by code run can be
translated into what we call an "item" of test paraphernalia like
a test example (or maybe even a shared setup method (experimentally)).
these paraphernalia are what do the actual producing of lines of test
code. this (and more) is expounded upon beginning at [#003] how nodes
are generated.

but we leverage the most power from the subject sidesystem by pointing
it at existing test files, which we call "synchronization"..




## experimental cool new feature 1: synchronization

(EDIT)




## experimental cool new feature 2: reverse synchronization

(EDIT)

_
