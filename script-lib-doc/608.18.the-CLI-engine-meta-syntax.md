# Introduction to the CLI "engine" and "frontend"

Yet another CLI overhaul. Many experiments here (all successful) synthesizing
all of our CLI efforts that came before it; all under the rubric of the
Alan Kay quote (popularized by Larry Wall):

    Simple things should be simple, complex things should be possible.

This was not designed in a vaccuum: it was forged purely from the necessity
of DRY-ing up (at writing) 8 different CLIs (with a pretty substantial set
of features, collectively). (The CLIs we refactored with this library are
tracked below under "How we use stacks to parse positional parameters".)

The ongoing (erm) _rubric_ of this library is that we not add features out
of pure novelty but rather of a perceived necessity from real-world use cases.


# Frontend/backend separation

We can make a frontend/backend separation: The backend does the actual parsing
of an ARGV using an abstract representation of the grammar; the various
available frontends (only one at writing, but a couple more are imagined)
parse representations of the grammars (or otherwise derive them) to produce
those abstract representations.

It's a useful separation because we can compartmentalize our development
efforts into steps and modularity: We can focus on what our parsers are
capable of doing separate from focusing all the different ways we want to
represent and derive syntaxes. Every time we come up with a cool new way to
make grammars, we won't have to scrap the engine.


# Overview from high-level to low:

We introduce ALTERNATION then SEQUENCE then the four kinds of fundamental
term types and then 8 or so actual term types we work with.

So, this explanation is top-down rather than bottom-up. The advantage of this
approach is that you see the big picture before the details. But the
disadvantage is that we'll be using some terms before we define them.


# ALTERNATION and SEQUENCE

All CLI's we will care to model, we can model syntactically as an ALTERNATION
of SEQUENCEs. (For simpler syntaxes, we won't need the ALTERNATION at all,
just a SEQUENCE, as we'll explain below.)

By "alternation" we mean "this or this or this.." It won't make sense how we
use ALTERNATION until after we explain SEQUENCEs but we'll introduce this high
level first:

                  + SEQUENCE
                 /
    ALTERNATION +-+ SEQUENCE
                 \
                  + SEQUENCE

Unlike our predecessor, there isn't a strong distinction between whether you're
modeling a "branch node" or a "leaf node": it used to be, you called a
different function to parse ARGV based on whether you had a "branch node" or a
"leaf node"; now, you call the same function. In effect you have a "branch node"
if your grammar is modeled with more than one usage line. But it's a distinction
we no longer see as important and one you can safely ignore.


## Not this

Conceivably we could construct parsers with more complex,
arbitrarily-deeply-nested recursive structures like this:

                                  + SEQUENCE
                                 /
                  + ALTERNATION +
                 /               \
    ALTERNATION +                 + SEQUENCE
                 \
                  + SEQUENCE

Fortunately we have never had occasion to want such a CLI parser and we hope
to continue not to. We suspect that the above can be "reduced" to a single
ALTERNATION with the three SEQUENCEs immediately childed to it; but we aren't
sure, and more broadly this discussion takes us well out of our scope here.

(We haven't explained what is a SEQUENCE yet.)


## Not this

    ALTERNATION + {empty list}

You must have at least one SEQUENCE in your alternation.


## An ALTERNATION of only one SEQUENCE is the same as the SEQUENCE alone

Not all CLI syntaxes require an ALTERNATION. Many simply require a SEQUENCE.
For such cases, the ALTERNATION node is superfluous.
As such, this:

    ALTERNATION +-+ SEQUENCE

gets reduced to this:

    SEQUENCE

That is, internally we substitute an ALTERNATION composed of one SEQUENCE with
just the SEQUENCE itself as the thing that parses the input. This is because
the only purpose of an ALTERNATION is to decide which of _several_ SEQUENCEs
to use to parse the ARGV. Reducing this moving part from the parse means less
code to debug for those grammars.


# A SEQUENCE is composed of four sections (canonically in this order)

Each SEQUENCE (specifically modeled for our practical CLI use-cases) has
four parts (conceptually, if not actually): `for_interactive`, `subcommands`,
`nonpositionals`, `positionals`. We'll introduce them quickly one-by-one here:


## SEQUENCE component: `for_interactive`

`for_interactive`: Whether this sequence is intended to parse from an
interactive terminal. The possible answers are True (yes), False (no), or
None (ignore the interactive-ness of the terminal). A value of False
indicates the generated parser is intended to consume only input piped in
thru STDIN (and will fail, explaining it to the user when it is not) etc.

If this distinction is not meaningful to you at this time, it is not
necessary to understand it at this time.


## SEQUENCE component: `subcommands`

You can skip the understanding of this concept if this is your first
introduction to CLI theory. For now we put all the documentation focuses
on this concept into this one section.

The `subcommands` section of a SEQUENCE is simply a sequence of zero or more
string literals that must exist (in that order, contiguously) in the actual
parameters from that point in the input; in practice anchored to the "head" of
ARGV when the invocation starts (but after the very first token of ARGV is
consumed, which is typically used to derive the `PROGRAM_NAME`).

This feature isn't very interesting or useful unless it's combined with
ALTERNATION: if you have an an ALTERNATION of various SEQUENCEs, each with
its own `subcommands`, then you have what we used to call a "branch node"
that (in effect) dispatches the processing of ARGV out to the intended
"endpoint".

Internally, the engine implements each SUBCOMMAND as a `required_positional`
on the positionals STACK (introduced below); each term having a
`value_constraint` (introduced below) that asserts that the token equals
exactly the string literal of the subcommand at that term.

(For at least one CLI, we have taken a more hand-written, custom,
(and probably more extensible) approach to subcommands; added in the same
commit as our #history-C.1)

(Fuzzy matching of SUBCOMMAND name is not yet a feature but we want to
leave an "upgrade path" to support it one day. For now it's a "nice to have".)


## SEQUENCE component: `nonpositionals`

A "floating cloud" of zero or more "nonpositionals". Typically you'll find
this category referred to elsewhere as "options" and/or "flags". We avoid
saying "options" here to avoid ambiguity. This concept is introduced more
formally below at "How we parse the nonpositionals with a floating cloud".


## SEQUENCE component: `positionals`

If we had to pick one, "positionals" are probably the essentialmost type of
formal parameter; at least as it pertains to whether or not you want to use
a library to parse your CLI.

Rolling your own stack-based parser for positional formals, every time your
grammar needs one, this might seem "fun" (for definitions of) the first one,
two or maybe three times; but at a certain point your soul will start to ache
every time you rewrite one of these from scratch.

There are multiple formal types that all fall under this umbrella-term
"positionals". Canonically (and also practially), we proscribe them in
this order:

1. Zero or more `required_positional` formal parameters
1. Zero or more `optional_positional` formal parameters
1. Zero or one "glob", either `required_glob` or `optional_glob`

Furthermore, you cannot have both an `optional_positional` and a
`required_glob` in the same syntax. More below.

Notes we can derive from the above:
- it's possible to have a syntax with *zero* positionals. Many do.
- The fact that optional positionals must follow required positionals, it is
  not set in stone; but for now we hard-code it this way in the spirit of
  keeping it simple, because it seems most idiomatic to the syntaxes we've
  seen in the real world (avoiding false requirements).

"The order in which term types are processed" below will expand on all this.


## Summary

With these four categories of formal parameter in mind, we can now zoom the
lens out a bit and explore some theory:


# Introducing our formal parameter theory

As offered above, "positionals" are perhaps the essentialmost foundational
principle of formal parameters in a CLI; if for no other reason than because
they require a parser "state" that needs to be maintained through the invocation
to parse them correctly, probably with a stack that probably solves for some
kind of modeling of "formal" parameters.

"Options" by comparison are somewhat more trivial (except when you get in to
some details): In a syntax with only nonpositionals, you might be able to get
away with parsing them "statelessly" as we do in some of our bash scripts with
`getopt` and while loops. Or perhaps you only need a boolean value to model
state, keeping track of whether you are expecting the value to a value-taking
option.

As such, this perceived centrality of "positionals" is reflected in our naming
of term types: We call all other formal parameters that are not positionals
"nonpositionals". Right now this includes only two kinds of formal parameters:
the `flag` and the `optional_nonpositional`. (This last term term (sic) may
change, as mentioned in a subsequent section.)

Here's one possible "taxonomization" of some term types suggested so far:

                             CLI formal parameters
                            /                    \
                           /                      \
                     positional               nonpositional
                      /     \                     /   \
                     /       \                   /     \
    required_positional  optional_positional   flag  optional_nonpositional


## Note: the dimensions are not universal

There is a lack of symmetry to the above tree: "requiredness" is a useful
dimension for positionals but not as much for nonpositionals; and whether the
formal takes an argument or not, this is relevant to nonpositionals but
nonsensical when applied to positionals. We "prove" this idea more fully below.


## Awkward name for an important type

Yes, `optional_nonpositional` is a mouthful of a name to have for perhaps
the commonmost type of formal parameter to have in a CLI, but we want a
name that is unambigous and fits in with the larger naming scheme.
Below we consider other names for this important type of formal parameter.
("Flag" is too strong an idiom for us to consider any other name for it.)


## Side note: We don't do "variable argument arity"

"value is optional" options not supported: Other generators in the world
generate parsers that allow options that take but do not require a value
(through use of an equal sign). We will not (for the time being) support
this variability: Either the option is a flag or it *does yes* take a
value. We consider the feature ("variable argument arity") to be superflous
but we want to allow room to include it later if we are convinced to.


## Why is there no `required_nonpositional`?

Again on the subject of symmetry, it's tempting to want to see a
`required_nonpositional` to compliment the all-important
`optional_nonpositional`. But it bears considering that the dimension of
required-ness does not impact the syntactic mechanics of the parser as it
pertains to nonpositionals, except in a trivial way. That is, to implement
required-ness for nonpositionals would not effect the underlying algorithm of
how ARGVs are parsed; rather, we need only assert requiredness at the end once
we have a parse tree.

Furthermore (and probably more importantly), required nonpositionals do not
occur very often in nature. Indeed, if you substitute the name "optional" for
"nonpositional", you get the oxy-moronic "required optional".


# On the distinction between positionals and nonpositionals

Broadly, as a first step in taxonomizing and characterizing different CLI's
(and their underlying "engines" (if any)); we suggest at first separating
the discussion of "options" versus [positional] "arguments"; because they are
(by design) fundamentally different in how they look, how they are are parsed;
and what characteristics and concerns come along with them.

(Indeed, we will observe this in the discussion in the next section, when
we see that not all characteristics relevant to positionals are relevant to
nonpositionals and vice-versa
("Development of the seven foundational term types" below).)

To parse positional arguments (or "positionals") we must keep track of which
formal parameters we have parsed already and which we are expecting next at
each step. Each next *actual* parameter gets lined up and stiched together to
a *formal* parameter. (For more on this, see
"How we use stacks to parse positional parameters" below.)

In contrast to positionals, the two kinds of nonpositionals
(`optional_nonpositional` and `flag` (introduced below)) may (begin to) match
an actual token at (theoretically) any point in the ARGV stream. This is why
we conceive of the formal nonpositionals as a "floating cloud" that travels
along with us as we parse the ARGV. (See
"How we parse the nonpositionals with a floating cloud" below for more.)

Indeed, from an implementation perspective, in our engine we effectively
manage these two concerns "in parallel" and "orthogonally"; that is, what's
happening over on the one side doesn't really effect the other and vice versa.

With these two broad categories of formal parameter in mind, we can now
dive in to the seven particular term types our engine works with:


# Development of the seven foundational term types

To create the foundation of our CLI syntax "theory"; we see what happens when
we permute several characteristics across each other. Imagine this initial set
of characteristics:
- Whether the formal field is "required" or "optional"
- Whether it is positional or nonpositional ("opt" vs "arg")
- Whether the field invocation can occur multiple-ly in one invocation
  (e.g. "glob"; e.g. a "flag" where the magnitude increases with repetition)
- If nonpositional, whether the field takes a value (or is just a "flag")

One way to arrive at six of our foundational seven (#here1) term types is to
permute these four binary characterics across each other:

         [non]pos | req/opt  |   many? |  value? | nonterminal name
       positional | required |  exac 1 |     n/a | required_positional
       positional | optional |  0 or 1 |     n/a | nested_optional_positionals
       positional | required |  1-many |     n/a | required_glob
       positional | optional |  0-many |     n/a | optional_glob
    nonpositional | required |    meh  |     yes | [not yet implemeted]
    nonpositional | optional |  maybe  |     yes | optional_nonpositional
    nonpositional | optional |  maybe  |      no | flag

(:#here1: The seventh foundational term type is "subcommand"; which in
practice we implement as a `required_positional` with a value constraint on it.
Because it's a higher-level language phenomenon that can be derived from these
lower-level concepts, we won't consider it explicitly after this point; but keep
in mind it's a relevant distinction to the engine.)

(Now there's an eighth foundational term type "stop parsing". There's no
documentation for it here (noted at `stop_parsing` below) but see (Case5950).)

Above shows seven rows. There should be 2^4 or sixteen rows. Why we don't
bother showing the missing nine rows is explored now:

Consider that our four categories are not cleanly orthogonal to each other in
practice: being "positional" implies "takes a value". So we could instead model
these two characteristics as a single taxonomy with three leaf nodes. As such
we'll munge "value?" and "[non]pos" together to make an ENUM-style field with
three possible values: `(positional, nonpos no value, nonpos takes value)`.

Furthermore a lower arity boundary (floor) of 0 implies "optional" and 1
implies "required". (We could just as well state this in the other direction:
"optional" implies a lower bounds of 0 and so on.) As such we'll simplify the
way we write the arities to simply "many" & "one". (We could just as soon
combine these two characteristics (optionality and many?) into a single enum
characteristic: `(0_1, 0_many, 1_1, 1_many)`; but we opt not to because of how
idiomatic, universal & understood is the concept of optionality
("required-ness") as its own characteristic.)

Let's now permute-out all these characteristics at this new moment,
with our revised 3 characteristics (this time 2x3x2 (12) permutations):

    required         positional            one   required_positional
    required         positional           many   required_glob
    optional         positional            one   nested_optional_positionals
    optional         positional           many   optional_glob

    optional     nonpositional no value    one   flag (but see #here2)
    optional     nonpositional no value   many   flag (but see #here2)
    required     nonpositional no value    one   flag (but see #here3)
    required     nonpositional no value   many   flag (but see #here3)

    optional   nonpositional takes value   one   optional_nonpositional (#here4)
    optional   nonpositional takes value  many   optional_nonpositional (#here4)
    required   nonpositional takes value   one   optional_nonpositional (#here5)
    required   nonpositional takes value  many   opt. nonpos. (#here4 & #here5)

Now notice all these footnote explanations this table requires:

:#here2: To have an arity characteristic on a flag is not "meta-syntactic":
In the wild, we *do* occasionally see a flag that is meaningfully "polyadic";
that is, that it is meaningful to repeat it (like see the "-v" option of the
python executable); however such a category of permutations is not modeled in
the "meta-syntax" (the syntax for parsing syntaxes); that is, we do not give
special representation in usage lines for those flags that can be meaningfully
repeated (just heuristically). We could (somehow) but we don't; because we've
never see it anywhere else; and we assume it happens not frequently enough for
it to be a formal language feature.

:#here3: Making a flag "required" is again a somewhat contrived permutation:
Almost axiomatically (if not tautologically), options are optional. To make an
option be required requires a leap of imagination: the '--force' (or '-f') flag
is sometimes required in some CLI operations; but again this is not a
meta-syntactic permutation: Our grammar for representing grammars does not have
any language features for modeling this permuation. We could but we don't
because it's a) not something we see in the wild and b) (relatedly) it's needed
so infrequently.

:#here4: The arity of a value-taking nonpositional is certainly interesting,
important and relevant; but FOR NOW it is not meta-syntactic: whether the
field acts like a list or a slot, this characteristic has no impact on how it
appears in usage lines. (It could, but it just doesn't because we haven't ever
seen this feature be expressed in any other usage lines elsewhere.)

:#here5: Similar to the perverse idea of a #here3 "required flag"; making a
required value-taking nonpositional is contrived. We have once in our life had
a use-case for this (a CLI for entering a database table row; the syntax
accepted option-looking name-value pairs; some columns were required) but again
this is certainly not meta-syntactic.

We can summarize the above footnotes to this: optionality and arity do not
meta-syntactically intersect with nonpositionals: you can have nonpositionals
with strange arities ("many") or strange optionalities ("required"); but these
contrived permutations do not get special recognition in the syntax for parsing
syntaxes. As such, we can simplify our permutations table down to this, most
practical distillation (which is not really one table but two (1x2x2 + 1x2)):

      positional required one        required_positional
      positional required many       required_glob
      positional optional one        nested_optional_positionals
      positional optional many       optional_glob
    nonpositional takes one value    optional_nonpositional
    nonpositional takes no value     flag

Again we do not develop the `subcommand` (macro) type through this approach
because it's above the scope of this methedology; but it bears keeping in mind
that it's the seventh term type.


# The order in which term types are processed

In review, here's the seven term types (expressed in plural because it
sounds more natural here):

- `subcommand`s
- `nonpositional`s (`flag`s and `optional_nonpositional`s)
- `required_positional`s
- `optional_positional`s (on surface, `nested_optional_positionals`)
- `required_glob` or `optional_glob` (mutually exclusive)

The above list also expresses the theoretical (if not also practical)
_order_ we want to process these formals in (both when processing actuals,
_and_ when producing lists of formals). Here's an explanation for each position:

`subcommand`s first: Intuitively, we do not know which commmand we are parsing
for until we know which command we .. are .. parsing for. So it follows that
we should parse for any subcommand first, before even parsing for
nonpositionals (but see discussion below on this).

`nonpositionals` before anything else (except subcommands): Pursuant to the
idea of a "floating cloud" of nonpositionals, ready to match an actual
nonpositional anywhere in the ARGV (see below); it follows that we should
parse (and represent) the nonpositionals first before the remaining
(positional) term types.

`required_positional`s before `optional_positional`s: It's possible to develop
parsers that "stitch" formals to actuals right-to-left rather than
left-to-right (see below near "stitching") (indeed some relatively recent
programming language versions allow this in their function signatures) BUT we
are not interested in such grammars at this time, because we don't recall
EVER seeing such a CLI grammar (optional positionals on the left) in the wild
_ever_; and to make such a feature opt-in would weigh-down our API without a
sufficient cost-benefit.

globs at the end: again we could make syntaxes that are left-globby rather
than right-globby; but we never see this in the wild. In practice, all the
CLIs we have ever seen that have a glob term have them as the finalmost
(rightmost) formal term of the syntax. We revist globs again in this section
below.

Multiple `optional_positional`s must nest: This is an interesting, derived
property of `optional_positional`s that should influence your syntax design:

Each next `optional_nonpositional` must piggy-back on any previous one.
To understand this, consider how the below nonsensical example of a syntax
is invalid:

    usage: {{prog_name}} [A] [B] [C]

The above suggests there could be an invocation where you only pass a value
for 'C' (but not 'A' or 'B') and so on. But, if only one actual positional
value is passed to the parser, how is it to know whether it "lines up with"
A, B, or C?

Rather, the closest you can come is:

    usage: {{prog_name}} [A [B [C]]]

But note this means that any invocation which passes a 'C' value must
also pass one for 'A' and 'B' and so on. In practice there are few command
interfaces that should want to align to such a constraint to any level of
depth past say 1 or 2, which is why if you find yourself wanting to use
three or more `optional_positional`s, you should probably use
`optional_nonpositional`s instead.

Now, there's an interesting interaction between the above language feature
of `nested_optional_positionals` and globs. First of all, it's possible to
have both in one syntax:

    usage: {{prog_name}} [A [B [C [D [D [..]]]]]]

The above says, "you can optionally pass an A (and optionally a B
(and then optionally a C (and any number of additional arguments, which
will be stored in the list "D")))".

Unfortunately, as we write this we realize that although it is possible to
produce a parser with the above grammar, the above syntax will not parse as
written. Rather, to get the above grammar you would have to write it like this
(even though the impossible syntax it suggests is not what it makes):

    usage: {{prog_name}} [A [B [C]]] [D [D [..]]]

The interesting interplay IS, you can have `optional_positional`s AND
globs so long as it's a `optional_glob` not a `required_glob`: It's hard to
explain in plain language, but it should be axiomatic that (given we support
only "right-globby" (introduced above) syntaxes), there can be no _optional_
formal positionals to the left of _required_ formal positionals.

But anyway, the uptake is, the seven types of terms are processed in this
order (theoretically when not also practically); for the reasons offered above.


# How we use stacks to parse positional parameters

We have written many argument parsers "by hand" that manage parsing
for one or more positional parameters. (The below table will capture
those CLIs that were once written by hand that now use "the engine".
You can look at each file and look for a history marker that can show
you the commit where they were changed from "by hand" to "engine-based"
to see what the code looked like when we wrote these parsers by hand.)

|       sub-project | identifier |  number of CLIs  |
|          kiss-rdb |  [#857.13] |   9 at writing   |


Having written so many by hand, we have
developed a favorite way to do so: represent these formal positionals (somehow)
on a stack, with each next positional that is expected being each next frame
on the stack.

    with each next token on the ARGV (frequently also turned in to a stack):
        if there's still more on the formal stack,
            pop the formal item off and use it to store the ARGV token,
            associating the token value with the name attached to the formal.
        otherwise (and the formal stack is now empty),
            fail with "unexpected argument: (etc)"

    (now that we exhausted ARGV)
    if there's still more on the formal stack,
        fail with "expecting (etc, formal argument name)"

We can implement trailing optional positionals by adding a boolean property to
the frames of the formal stack, and checking for this boolean value at the
final `if` clause above.

We can implement globs by adding yet another boolean property to the formal
structure and checking for it before we `pop` (and not popping off if it's
a glob.)

To implement these above two meta-properties correctly we will have to assert
that their particpating frames (formals) are in a sensical position relative
to one another (i.e at the end of the line of formals, or the bottom of the
stack.)


# How we parse the nonpositionals with a floating cloud

In the generated parser, we (by default, and at writing always) parse
"nonpositionals" in what we consider to be the "least surprise", commonmost
way; where they can occur alsmot anywhere vis-a-vis the positionals. (We will
put a finer point on this later in this section).

We *could* generate parsers that follow to the letter the positionality
suggested by the usage line; that is, imagine the syntax suggested by:

    usage: {{prog_name}} [--flag1] [--opt-nonpos1=X] ARG1 [-x] [-y] ARG2

Is it that the `-x` and `-y` flags are recognized only between the actual
tokens for ARG1 and ARG2? We think not. In practice, we see it as a commonly
understood idiom that nonpositionals can occur before, after, or in the
middle of the run of positionals.

This is why in the implementation of the engine we refer to this as a
"floating cloud" of formal nonpositionals: the "cloud" of possible
nonpositionals "floats along" the parsing of the ARGV.

This is in stark contrast to the stack-like parsing of positionals,
where where (sic) you are in the ARGV has a big impact on whether and how
each next positional-looking token is parsed.

It's worth mentioning that in the above, we are talking about where the
_actual_ parameters (ARGV tokens) may fall; not how the _formal_ parameters
are placed with respect to each other in (say) the usage line. The above
section "The order in which term types are processed" suggests a formal order
for the frontend to follow; but officially this is up to the frontend what
surface order they want their terms to be in.

Now, interestingly this can be true or not true when it comes to
nonpositionals vis-a-vis the chain of subcommand literals. What we imagine
to be the "least surprise" way would be that you can't start using
nonpositionals until you know what subcommand is invoked (because each
subcommand has its own associated nonpositionals (and positionals).

Remarkably, the above constraint would not necessarily have to be so:
XX we won't bother explaining this until we have a test demonstrating the
idea (that there's no reason you can't start parsing for a subcommand's
nonpositionals before parsing the subcommand literals -- except that it
could lead to messy fail states if you have subcommands that assign different
constraints or syntactic categories to the same surface names...)


## Hot-swappable grammars and the floating cloud

In the first (and at writing only) frontend, we require that you represent
your grammar such that all formal nonpositionals occur contiguously, in a
certain "place" in the usage line (relative to terms of other categories).
(This order was proscribed in the section above, called
"The order in which term types are processed".)

It would be semi-trivial (and fun?) to generate parsers that add
nonpositionals to the "floating cloud" one-by-one as they are encountered
in the syntax (that is, the surface representation of the grammar), and only
add clusters of contiguous terms to the "cloud" as we parse from ARGV the
formal positionals that nest those clusers.

However, we have no interest or need in attempting such a stunt at this time.

Hot-swappable, live-loading of the _positional_ parts of a grammar is also
an option; but again one that is out-of-scope for this phase of the development.


# "BSD-style" nonpositionals

We refer to this "mode" of parsing as "BSD-style" because we first noticed
it in our `find` utility which happeed to come from BSD (although this "mode"
is also at play in the GNU variant of `find`).

What it means is simply that we use _one_ dash instead of _two_ for
"long" names, and as a consequence we can't have "ball of mud" surface
tokens that select many options at once. Huh?

## example of not "BSD-style":

These are the same:

`my-prog --verbose --force`

and:

`my-prog -vf`

We refer to the above surface form as a "ball of mud". The single dash is
followed by a run of characters, _each character_ of which selects the short
form of an option.

## example of "BSD-style":

`my-prog -verbose -force`

In BSD-style, as an aesthetic choice we want only one dash and not two
before the long-surface-forms. The benefit of this is the aesthetic joy
in seeing only one dash; but the cost is that we can't do "ball of mud"
surface forms.

## synthesis & conclusion

At writing, the state of things is undefined for the following issue because
we can't point you to a specific test case that illustrates this; but how we
imagined it in our heads when we wrote it is that BSD-style parsing "turns on"
when your syntax has _one_ or more "familiar name" that employs the BSD-style
name (only one dash).

There are many questions we don't have concrete answer to yet:
- Can you mix the two styles? We think that yes you can have both single-dash
  and double-dash familiar names in your syntax; but if you have one or more
  single-dash-named familiar names, we won't parse single-dash-looking tokens
  with the "ball-of-mud" approach.
- How does fuzzy-matching intersect with this? We don't know at the moment.
- Do we have plans to allow for "proper" short forms? We don't know at this
  time.

We track this phenomenon with [#608.20] so see those examples for further
exploration. (At writing these are all in [kiss-rdb].)


# Value constraints and value normalizers

The engine's extensibility (to the extent that it is extensible) comes largely
from the formal parameter structures' validation/normaliztion API. The formal
parameter structures have non-required properties that hold functions for value
validation and normalization.

There are currently two such properties that formal structures may have
(and not all formal structures will have all these properties):
`value_constraint` and `value_normalizer`. The next two sections cover these.


## Value constraints

The value _constraint_ is a function that takes simply the ARGV "head" token
(the one at the front) and (in effect) returns a "yes" or a "no" based on
whether the token is valid.

But it's a bit more complicated than that, in a way that will introduce
a foundational concept to the engine: _response structures_. For the value
constraint function to say "yes" (the token is valid), it returns simple
`None`. For the function to say "no" (the token is not valid), it returns a
"response tuple" of type `early_stop`. Within this response structure is the
_capability_ of expressing the issue with the token. (But this expression
is not emitted at the time it is created.) Response tuples are introduced
in a nearby section below.

(As a historical footnote, we took the name (and some of the idea) from
[recutils][rec1].)

The unit tests concerning this feature are the authoritative API documentation
for it.


## Value normalizers

Our conception of a value _normalizer_ is similar to that of a value
_constraint_ but more complicated/powerful: Whereas a constraint simply says
whether or not the head token is valid to be the thing, the normalizer can do
this and also determine how the value is stored (including but not limited to
what value is used).

A contrived but comprehensively didactic example is a CLI that takes a list
of IP addresses as optional arguments. Our imagined requirements for this
formal parameter are three:

1. The token (string) must match some particular regex for IP addresses.
   (When it fails to match, we require a custom expression of failure.)
1. We don't want to store the value into the parse tree as a string but as
   some arbitrary business value (say, a tuple of four integers)
1. The end user can pass multiple of them and so we want the resultant
   parse tree to have them in a list or tuple

The _current_ way to meet these requirements (and this is subject to get easier)
is to use a `value_constraint` in conjunction with a `value_normalizer`.
(It's possible to do everything with the normalizer but we prefer to keep
constraint-like behavior in a constraint.)

If the token matches the required pattern (say), the value constraint returns
a `value_constraint_memo` result tuple (whose value is always an `re.Match`
object in practice). Otherwise, it should return an `early_stop` of sub-type
`failed_value_constraint`.

The above is not enough to explain it all on its own. (Case5954) is the
authoritative reference on this subject


## Is it necessary to have constraints and normalizers as separate concepts?

No. In theory, the "normalizer" can do everything the "constraint" can do.
But we wanted the "constraint" as a separate concept because it's easier to
understand and has a more obvious, intuitive API; we wanted this API to be
exposed for those use cases that only need a constraint and not a normalizer.

The constraint/normalizer dichotomy is not set-in-stone. It is subject to
change.


## Limitations, issues & nice-to-haves w/ normalizers & constraints

### Imagine normalizer chains

One [nice-to-have][wiki3] would be that internally, there is only a
`normalizer_chain` property. Some combination of the syntax designer and
the library would have to reconcile what order the normalizer functions
occur in (append to end or insert at beginning, probably).


# sexp's as the grammar "lingua franca" of our engine

The main job of the "frontend" is to take some kind of input from which a
grammar can somehow be derived, and to derive that grammar and pass it back
to the "engine" along with the ARGV to actually do the parsing and (when
no syntax errors) to produce a parse tree to pass back to the user.

    (user)                    (frontend)              (backend ("engine"))

    [some kind of
     representation   ---->   [do work to parse
     of a grammar]             the grammar           ----> [do work to
                               ("parse parsing")]           make a parser]

                                                   _________/
                             [have the parser]  <-/


            ARGV      ---->   [parse ARGV with   --------> [parse the ARGV
                               the parser]                  with the engine
                                           <-------------  against the grammar]
                                /
     [the parse tree]  <-------/

So this is to say, the main job of the frontend is to do "parse parsing",
that is, parsing some input from which a grammar can be derived, and passing
that grammar back to the backend along with the ARGV.

The question becomes, in what manner do we represent the grammar once it has
been parsed, in some format the backend can use?

One approach would be to use classes exposed by the backend: maybe a `Flag`
class and an `ArgumentedOption` class and a `Positional` class. (From these
three, we would be able to derive all seven "foundational types" by giving
some of the classes properties like `is_required` and `is_glob`.)

But consider the [dependency inversion principle][wiki1]:

> High-level modules should not import anything from low-level modules.
> Both should depend on abstractions (e.g., interfaces).

Applying this concept to our "engine"/"frontend" system here, we decided
to use (our version of) [S-expressions][wiki2] as the "lingua franca"
for representing grammars when passing them to the engine. (We call
them "sexps" (we prounce this "sex pee's")).

The below pseudocode is an attempt to document (comprehensively)
our grammar-grammar; that is, how we represent the seven foundational term
types in sexps. It seems likely that our *actual* API for
this has wandered from this initial design; but only somewhat.
The relevant unit tests are the authoritative API reference on the subject.

But here's the sketch:

    "?" means zero or one. These ones provided in pairs. any order
    "D" means it's a property derived from others (hard-coded for now)

    { required_positional | optional_positional }  # either
    { <familiar_name str> | <familiar_name_function callable> }
    ? <value_constraint>
    ? <value_normalizer>
    ? 'glob'  # a tuple of one element, the string 'glob'
    D parse_tree_key

    optional_nonpositional  # and required_nonpositional if we ever do that
    <familiar_name>  # must start with one or two dashes
    <parameter_familiar_name>  # SHOUTCASE or <these>
    ? <value_constraint>
    ? <value_normalizer>
    D parse_tree_key

    flag
    <familiar_name>
    ? <value_normalizer>
    D parse_tree_key

"flag" is the only formal parameter type that doesn't take a value. As such,
it doesn't model a `value_constraint` optional property like the others do.
However, it does allow for a `value_normalizer`, which can change how the
values are written to the parse tree.


# The internal parsing API of the engine

We call the act of parsing a particular ARGV input against our grammar an
INVOCATION.

For an INVOCATION, from each SEQUENCE we derive an FSA (finite state
automaton). Details of the states of this FSA will be divulged below; but one
thing this FSA does is it "points at" which formal positional parameter (if
any) is expected/allowed next. (The way positional parameters are processed is
more state-machine-ey than how nonpositionals are parsed; but also there is
need for state with nonpositionals: consider the `optional_nonpositional` in
the state when the value is expected.)


## Introducing the "in the running" dictionary

We implement parsing by issuing a series of two or more "events" (more below
on which two) to the FSA's in THE\_RUNNING. The FSA indicates whether it
ACCEPTs or FAIL\_TO\_ACCEPTs the event with either `None` or a RESPONSE\_TUPLE
strucure (respectively) from its RECEIVE\_INPUT\_EVENT method.

More below about what we do based on various patterns of the FSAs
in the THE\_RUNNING accepting or failing to accept each next input event.

The "at least two events" are the IS\_INTERACTIVE event (indicating whether
the terminal is interactive or not), and the END\_OF\_TOKENS event. For the
first, FSA's that don't care about the interactive-ness should return None
(they accept). The END\_OF\_TOKENS event will be the time where FSA's whose
grammars have `required_positional` will determine if a `required_positional`
is being pointed at.

At each step after the input event has been distributed out to every FSA
in the running and we have their responses, the set of their responses falls
into one of these N groups:
1. some accepted: zero or more FAIL\_TO\_ACCEPTed, but one more more ACCEPTed
2. none accepted: all FAIL\_TO\_ACCEPTed
We discuss what to do in these two cases in the next two points.

When some accepted, we take those FSA's that FAIL\_TO\_ACCEPTed *out* of
THE\_RUNNING. We do not need to keep them for any future possible
error-reporting; we are just focusing on the FSA's still in the running.

Each FSA must be able to express what it is expecting. When none ACCEPTed
the input event, we will express this as a "parse failure" condition.
Each FSA in the running (they all FAIL\_TO\_ACCEPTed) will have resulted
in a RESPONSE\_TUPLE of type `early_stop`. The second component of this tuple
will be a function that when called produces metadata explaining the reason
for the parse failure.

When failing against the IS\_INTERACTIVE event, the failure to accept
must be because they expected (required) the inverse of the boolean value.
(They required interactive but it was not, or the opposite).

Failing to accept at END\_OF\_TOKENS will always (as far as we can imagine
at this writing) be either:

- `required_positional` expected (is being "pointed at") OR
- `optional_nonpositional` requires a value

Failing to accept a HEAD\_TOKEN, we anticipate it's these N cases:

- Token is OPTION\_LOOKING (starts with dash) but doesn't match
  against the FLOATING\_CLOUD\_OF\_NONPOSITIONALS.
- Token does _not_ look like an option (does not start with a dash),
  may have failed either because:
  - no more positional arguments are being accepted from the FSA in
    its current state ("pointing at" the end)
  - A failure of a `value_constraint` or `value_normalizer`...
  - for example, the formal positional being pointed at is a string literal
    (SUBCOMMAND) and the argument token does not equal it.

For failure of an OPTION\_LOOKING token against the FLOATING\_CLOUD, at present
we will just fail with a generic failure ("unrecognized option '--foo'") but
it's conceivable that in the future we will want to do a fuzzy match
type traversal for suggestions ("did you mean '--foz' or '--flu'?"). For
this generic failure against an OPTION\_LOOKING HEAD\_TOKEN, the
STRUCTURED\_FAILURE\_EXPLANATION may be a generic singleton value..

Another interesting error condition we may encounter is ambiguity in which
FSA to use if more than one are still in THE\_RUNNING by the END\_OF\_TOKENS
event. In real life, grammars are never made with such ambiguity in them
(just as it works out) so this is a failure state we won't spent a lot of
time making very pretty.


## Things we have yet to explain (about the engine's internal API)

- short surface forms
- how the parse tree is actually written to
- `stop_parsing`
- the [#608.S] #wish of #feature:just-in-time-parse-parsing


# Merging responses

It's essential that the engine be able to exhibit behavior something
like this: Take multiple responses from the multiple components of an
alternation and merge them in to a single, unified, merged response.

The target use case is where we have a set of subcommands modeled by a
corresponding set of usage lines: if ARGV doesn't match any of the subcommands,
each subcommand reports its "expecting" surface expression. Merge these
responses together to express a unified message "expecting 'A', 'B' or 'C'".

It's possible that the input responses are "polymorphic", i.e., of different
subtypes: you may have a subcommand match failure from one usage line and a
value constraint failure from another. In such cases, it's still possible to
merge these two ideas with a single expresion because their expression
strategies are similar (even if it may sound awkward):
"expecting 'beef' or AGE"

We attempt polymorphic merging of subtypes by placing each subtype somewhere
in a unified "semantic taxonomy" (tree), and finding the specific-most subtype
that still pertains to all the inputs:

Say we are combining "'98 Ford Mustang" and "'67 Ford Mustang": with each
of these subtypes we associate an expanded "channel":
    "'98 Ford Mustang" => ('car', 'ford', 'mustang', '1998')
    "'67 Ford Mustang" => ('car', 'ford', 'mustang', '1967')

We do the "Longest Common Sequence" algorithm on the two "channels" to
get ('car', 'ford', 'mustang'), of which we take the specific-most component
'mustang' to be the expression of subtype we'll use.

But if we also had the third input:

    "2013 Dodge Challenger" => ('car', 'dodge', 'challenger', '2013')

then the longest common sequence would leave us with only 'car'.

It makes sense to combine some subtypes but not others: it "feels"
nonsensical to try to combine a "display help" early stop with an
"expecting"-style early stop. In such cases, the engine sees the onus as
being on the grammar to make it impossible for such amibuous parse states
to ever be reached.


[recu1]: https://www.gnu.org/software/recutils/manual/Arbitrary-Constraints.html
[wiki1]: https://en.wikipedia.org/wiki/Dependency_inversion_principle
[wiki2]: https://en.wikipedia.org/wiki/S-expression
[wiki3]: https://en.wiktionary.org/wiki/nice-to-have


## (document-meta)

- #history-C.1
- #born
